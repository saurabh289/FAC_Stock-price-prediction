{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FAC_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOW5arFQHW+OAR2rbL17Guk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabh289/FAC_Stock-price-prediction/blob/main/FAC_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW318pfJCeH"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DecEcoBSJKMY"
      },
      "source": [
        "is_ipython = 'inline'  in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHxqORSSJMsK"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=input_size,out_features=64)\n",
        "    self.fc2 = nn.Linear(in_features=64,out_features=8)\n",
        "    self.out = nn.Linear(in_features=8,out_features=3)\n",
        "  def forward(self,t):\n",
        "    t = t.flatten(start_dim=1)\n",
        "    t = F.relu(self.fc1(t))\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = self.out(t)\n",
        "    return t  \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_2BkAmBJMyE"
      },
      "source": [
        "Experience = namedtuple('Experience',('state','action','next_state','reward'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ToEWVTAJTp-"
      },
      "source": [
        "class ReplayMemory():\n",
        "  def __init__(self,capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "    self.push_count =0\n",
        "  def push(self,experience):   \n",
        "    if len(self.memory)<self.capacity:   #check if it's less than the memory's capacity\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count%self.capacity] = experience   #store in front of memory\n",
        "      self.push_count+=1\n",
        "  def sample(self,batch_size):\n",
        "    return random.sample(self.memory,batch_size)\n",
        "  def can_provide_sample(self,batch_size):\n",
        "   # print(len(self.memory))\n",
        "    return len(self.memory)>=batch_size  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBc5_NwbJVyy"
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "  def __init__(self,start,end,decay):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.decay = decay\n",
        "  def get_exploration_rate(self,current_step):\n",
        "    return self.end + (self.start-self.end)*math.exp(-1.*current_step*self.decay)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1omflFaiJYZ5"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self,strategy,num_actions,device):\n",
        "    self.current_step = 0 \n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "    self.device = device \n",
        "  def select_action(self,state,policy_net):\n",
        "    rate = strategy.get_exploration_rate(self.current_step)\n",
        "    self.current_step +=1\n",
        "    if rate> random.random():\n",
        "      action =  random.randrange(self.num_actions)  # explore\n",
        "      return torch.tensor([action]).to(device)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        #print(state,\"f1\")\n",
        "        return policy_net(state).argmax(dim=1).to(device) # exploit"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNzLIvk9JdO5"
      },
      "source": [
        "\n",
        "def formatPrice(n):\n",
        "    return(\"-Rs.\" if n<0 else \"Rs.\")+\"{0:.2f}\".format(abs(n))\n",
        "def getStockDataVec():\n",
        "    vec = []\n",
        "    lines = open(\"/content/NFLX.csv\",\"r\").read().splitlines()\n",
        "    for line in lines[1:2267]:\n",
        "        #print(line)\n",
        "        #print(float(line.split(\",\")[4]))\n",
        "        vec.append(float(line.split(\",\")[4]))\n",
        "        #print(vec)\n",
        "    return vec \n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def getState(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        res.append(sigmoid(block[i + 1] - block[i]))\n",
        "    return np.array([res])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AG6JqREL8Nz"
      },
      "source": [
        "def plot(values, moving_avg_period):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(values)\n",
        "    plt.plot(get_moving_average(moving_avg_period, values))\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython: display.clear_output(wait=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsKUn4yMAdi"
      },
      "source": [
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpO_L14MIYa"
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "\n",
        "    t1 = torch.cat(batch.state)\n",
        "   # print(t1)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.reward)\n",
        "    t4 = torch.cat(batch.next_state)\n",
        "\n",
        "    return (t1,t2,t3,t4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRDQoTIWxi-"
      },
      "source": [
        "class QValues():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def get_current(policy_net, states, actions):\n",
        "      return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "    def get_next(target_net, next_states):\n",
        "      final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "      non_final_state_locations = (final_state_locations == False)\n",
        "      non_final_states = next_states[non_final_state_locations]\n",
        "      batch_size = next_states.shape[0]\n",
        "      values = torch.zeros(batch_size)#.to(QValues.device)\n",
        "      values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "      return values  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P71-1ogEMN-G"
      },
      "source": [
        "batch_size = 256\n",
        "gamma = 0.999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update  = 10\n",
        "memory_size = 100000\n",
        "lr = 0.01\n",
        "num_episodes =30\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67wezQmMP0h"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#em = CartPoleEnvManager(device)\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent = Agent(strategy, 3, device)\n",
        "memory = ReplayMemory(memory_size)\n",
        "vec = getStockDataVec()\n",
        "k=len(vec)\n",
        "#print(k)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aagr4VUHMSH9"
      },
      "source": [
        "\n",
        "policy_net = DQN(64).to(device)\n",
        "target_net = DQN(64).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "window_size = 64"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR-sXNPFNTpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036ec51e-3d9e-4ebc-8439-81cb48bef232"
      },
      "source": [
        "episode_durations = []\n",
        "for episode in range(num_episodes):\n",
        "    print(\"Episode \" + str(episode) + \"/\" + str(num_episodes))\n",
        "    state = torch.tensor(getState(vec, 0, window_size + 1))\n",
        "    total_profit = 0\n",
        "    invent = []\n",
        "    #state = getState(vec,)\n",
        "    max_transaction = 20 \n",
        "    total_money = 10000\n",
        "    c_s_h = 0\n",
        "    c_t_c =0\n",
        "    for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(1,action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "         #print(\"Buy: \" + formatPrice(data[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "        reward = -vec[t]*b_p[0]+b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      if memory.can_provide_sample(batch_size):\n",
        "        experiences = memory.sample(batch_size)\n",
        "        states, actions, rewards, next_states = extract_tensors(experiences)\n",
        "        current_q_values = QValues.get_current(policy_net, states.float(), actions)\n",
        "        next_q_values = QValues.get_next(target_net, next_states.float())\n",
        "        target_q_values = (next_q_values * gamma) + rewards\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        # episode_durations.append(t)\n",
        "        # plot(episode_durations, 100)\n",
        "         break\n",
        "    print(total_profit)    \n",
        "    if episode % target_update == 0:\n",
        "      target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/30\n",
            "1877.3155676000397\n",
            "Episode 1/30\n",
            "1392.3287290791218\n",
            "Episode 2/30\n",
            "3163.975707737608\n",
            "Episode 3/30\n",
            "14.678499317016247\n",
            "Episode 4/30\n",
            "64.24857130071604\n",
            "Episode 5/30\n",
            "226.6588992914958\n",
            "Episode 6/30\n",
            "-211.6311585845266\n",
            "Episode 7/30\n",
            "-77.70356553463529\n",
            "Episode 8/30\n",
            "50.11834192462993\n",
            "Episode 9/30\n",
            "623.1368133885258\n",
            "Episode 10/30\n",
            "-80.19480076780104\n",
            "Episode 11/30\n",
            "250.26866117377074\n",
            "Episode 12/30\n",
            "477.80663825126896\n",
            "Episode 13/30\n",
            "172.10420658087287\n",
            "Episode 14/30\n",
            "247.81576687548\n",
            "Episode 15/30\n",
            "254.41735722176276\n",
            "Episode 16/30\n",
            "95.98140454079476\n",
            "Episode 17/30\n",
            "26.04102722339678\n",
            "Episode 18/30\n",
            "-3.6480487050531565\n",
            "Episode 19/30\n",
            "47.5909590457249\n",
            "Episode 20/30\n",
            "227.6119867079829\n",
            "Episode 21/30\n",
            "224.196354323116\n",
            "Episode 22/30\n",
            "291.4910621000385\n",
            "Episode 23/30\n",
            "182.37426702450642\n",
            "Episode 24/30\n",
            "79.79279927095541\n",
            "Episode 25/30\n",
            "183.4150735644502\n",
            "Episode 26/30\n",
            "198.63567247024042\n",
            "Episode 27/30\n",
            "82.02794510924195\n",
            "Episode 28/30\n",
            "116.22244035074294\n",
            "Episode 29/30\n",
            "117.95392055076707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pToT-UfEO8pM",
        "outputId": "7b34e5b2-32a6-48a7-e1f6-a6bfdd168de3"
      },
      "source": [
        "print(total_profit)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117.95392055076707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdXC5vgVTeNw",
        "outputId": "b15f104d-1176-4279-966f-aafee46c13b5"
      },
      "source": [
        "max_transaction = 50\n",
        "total_money = 10000\n",
        "c_s_h = 0\n",
        "c_t_c =0\n",
        "print(k)\n",
        "for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "        print(\"Buy: \" + formatPrice(vec[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        print('profit :', total_profit)\n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action ==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "       # print('f1')\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      \n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        #episode_durations.append(timestep)\n",
        "        #plot(episode_durations, 100)\n",
        "        break\n",
        "print(total_profit)\n",
        "print(total_money)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n",
            "Buy: Rs.357.12\n",
            "profit : 120.55249529593081\n",
            "Buy: Rs.488.28\n",
            "profit : 115.41475971767332\n",
            "Buy: Rs.532.39\n",
            "profit : 122.32521650130104\n",
            "Buy: Rs.516.39\n",
            "Buy: Rs.518.02\n",
            "Buy: Rs.508.05\n",
            "profit : 131.24871789029905\n",
            "profit : 145.65212977116906\n",
            "profit : 160.81080156459987\n",
            "Buy: Rs.502.81\n",
            "profit : 170.50192939770662\n",
            "170.50192939770662\n",
            "10052.548008846934\n"
          ]
        }
      ]
    }
  ]
}